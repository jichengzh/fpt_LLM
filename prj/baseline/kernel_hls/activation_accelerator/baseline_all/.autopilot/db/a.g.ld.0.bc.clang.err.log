remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/src/c++/expfloat.cpp:22:9: Inlining function 'exp_reduce_::exp(float)' into 'hls::expf(float)'
remark: ./bf16_accl.h:269:12: Inlining function 'hls::expf(float)' into 'f32_expf(float)'
remark: activation_accelerator.cpp:440:44: Inlining function 'hls::expf(float)' into 'float_silu2(float const*, unsigned short*, int)'
remark: activation_accelerator.cpp:435:9: Loop 'silu_inner' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:688:0: Unrolling loop 'add_inner' (activation_accelerator.cpp:696:9) in function 'float_add2<64, 768>' completely with a factor of 64
remark: activation_accelerator.cpp:967:0: Unrolling loop 'multiply_inner' (activation_accelerator.cpp:975:9) in function 'float_Multiply2<64, 768>' completely with a factor of 64
remark: activation_accelerator.cpp:426:0: Unrolling loop 'silu_inner' (activation_accelerator.cpp:435:9) in function 'float_silu2' completely with a factor of 32
remark: activation_accelerator.cpp:539:0: Unrolling loop 'normalize_inner_rms_norm3' (activation_accelerator.cpp:573:9) in function 'float_rms_norm3' completely with a factor of 64
remark: activation_accelerator.cpp:539:0: Unrolling loop 'rms_calculate_loop_rms_norm3' (activation_accelerator.cpp:563:5) in function 'float_rms_norm3' completely with a factor of 64
remark: activation_accelerator.cpp:539:0: Unrolling loop 'init_y_sum_and_rms_sq' (activation_accelerator.cpp:554:5) in function 'float_rms_norm3' completely with a factor of 64
remark: activation_accelerator.cpp:587:0: Unrolling loop 'normalize_inner_layer_norm3' (activation_accelerator.cpp:644:9) in function 'float_layer_norm3' completely with a factor of 64
remark: activation_accelerator.cpp:587:0: Unrolling loop 'std_blocks_layer_norm3' (activation_accelerator.cpp:635:5) in function 'float_layer_norm3' completely with a factor of 64
remark: activation_accelerator.cpp:587:0: Unrolling loop 'mean_blocks2_layer_norm3' (activation_accelerator.cpp:627:5) in function 'float_layer_norm3' completely with a factor of 64
remark: activation_accelerator.cpp:587:0: Unrolling loop 'mean_inner_layer_norm3' (activation_accelerator.cpp:620:9) in function 'float_layer_norm3' completely with a factor of 64
remark: activation_accelerator.cpp:587:0: Unrolling loop 'init_partial_layernorm' (activation_accelerator.cpp:607:5) in function 'float_layer_norm3' completely with a factor of 64
remark: ./bf16_accl.h:362:0: Unrolling loop 'sum_square2' (./bf16_accl.h:378:5) in function 'square' completely with a factor of 64
remark: ./bf16_accl.h:362:0: Unrolling loop 'sum_inner_square' (./bf16_accl.h:371:9) in function 'square' completely with a factor of 64
remark: activation_accelerator.cpp:910:0: Unrolling loop 'lane_loop' (activation_accelerator.cpp:922:9) in function 'row_norm_store_hls<64, 768>' completely with a factor of 64
remark: activation_accelerator.cpp:875:0: Unrolling loop 'exp_inner' (activation_accelerator.cpp:895:9) in function 'row_exp_bucket_sum<64, 768>' completely with a factor of 64
remark: activation_accelerator.cpp:875:0: Unrolling loop 'init_partial' (activation_accelerator.cpp:880:5) in function 'row_exp_bucket_sum<64, 768>' completely with a factor of 64
remark: activation_accelerator.cpp:848:0: Unrolling loop 'lane_reduce' (activation_accelerator.cpp:864:9) in function 'row_max_hls<64, 768>' completely with a factor of 64
remark: activation_accelerator.cpp:848:0: Unrolling loop 'init_lane_max' (activation_accelerator.cpp:853:5) in function 'row_max_hls<64, 768>' completely with a factor of 64
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_isnan.h:16:0: Inlining function 'fp_struct<float>::fp_struct(float)' into 'int generic_isnan<float>(float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/src/hls/utils/x_hls_utils.h:351:0: Inlining function 'fp_struct<float>::data() const (.48.55.61.69.75.83.89.97.103.111)' into 'fp_struct<float>::to_float() const'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/src/hls/utils/x_hls_utils.h:375:0: Inlining function 'fp_struct<float>::to_float() const' into 'fp_struct<float>::to_ieee() const'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/src/hls/utils/x_hls_utils.h:348:0: Inlining function 'fp_struct<float>::data() const (.48.55.61.69.75.83.89.97.103.111)' into 'fp_struct<float>::to_int() const'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'fp_struct<float>::fp_struct(float)' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'int generic_isnan<float>(float)' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'fp_struct<float>::to_ieee() const' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'fp_struct<float>::to_int() const' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/src/c++/fmaxfloat.cpp:10:0: Inlining function 'hls::fmax(float, float)' into 'hls::fmaxf(float, float)'
remark: activation_accelerator.cpp:1020:0: Applying array_partition to '_ZZ22activation_acceleratorPtS_S_iiE4buf2': Block partitioning with factor 64 on dimension 1.
remark: activation_accelerator.cpp:941:8: Applying array_partition to 'exp_buf': Block partitioning with factor 64 on dimension 1.
remark: activation_accelerator.cpp:946:8: Applying array_partition to 'max_row': Complete partitioning on dimension 1.
remark: activation_accelerator.cpp:949:8: Applying array_partition to 'sum_row': Complete partitioning on dimension 1.
remark: activation_accelerator.cpp:596:11: Applying array_partition to 'y_sum_sq': Complete partitioning on dimension 1.
remark: activation_accelerator.cpp:544:11: Applying array_partition to 'y_sum_sq': Complete partitioning on dimension 1.
remark: activation_accelerator.cpp:1021:11: Applying array_partition to 'x': Block partitioning with factor 64 on dimension 1.
remark: activation_accelerator.cpp:1021:22: Applying array_partition to 'y': Block partitioning with factor 64 on dimension 1.
remark: activation_accelerator.cpp:1039:9: Sequential read of length 49152 has been inferred _XLX_SEP_ OldID=for.inc.load.4,  _XLX_SEP_ in0seq in0 gmem0 stage_0_load0 activation_accelerator.cpp:1039:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:1043:9: Sequential read of length 49152 has been inferred _XLX_SEP_ OldID=for.inc13.load.4,  _XLX_SEP_ in1seq in1 gmem1 stage_0_load1 activation_accelerator.cpp:1043:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:1122:9: Sequential write of length 49152 has been inferred _XLX_SEP_ OldID=for.inc81.store.136,  _XLX_SEP_ outseq out gmem2 stage_2_store activation_accelerator.cpp:1122:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:1043:9: Could not widen since type i16 size is greater than or equal to the max_widen_bitwidth threshold of 0 _XLX_SEP_ in1seq in1 gmem1 stage_0_load1 activation_accelerator.cpp:1043:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:1039:9: Could not widen since type i16 size is greater than or equal to the max_widen_bitwidth threshold of 0 _XLX_SEP_ in0seq in0 gmem0 stage_0_load0 activation_accelerator.cpp:1039:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:1122:9: Could not widen since type i16 size is greater than or equal to the max_widen_bitwidth threshold of 0 _XLX_SEP_ outseq out gmem2 stage_2_store activation_accelerator.cpp:1122:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:1039:9: Multiple burst reads of length 49152 and bit width 16 has been inferred. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings. _XLX_SEP_ seq1  gmem0 stage_0_load0 activation_accelerator.cpp:1039:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:1039:9: Multiple burst reads of length 49152 and bit width 16 in loop 'stage_0_load0'(activation_accelerator.cpp:1039:9) has been inferred on bundle 'gmem0'. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
remark: activation_accelerator.cpp:1043:9: Multiple burst reads of length 49152 and bit width 16 has been inferred. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings. _XLX_SEP_ seq2  gmem1 stage_0_load1 activation_accelerator.cpp:1043:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:1043:9: Multiple burst reads of length 49152 and bit width 16 in loop 'stage_0_load1'(activation_accelerator.cpp:1043:9) has been inferred on bundle 'gmem1'. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
remark: activation_accelerator.cpp:1122:9: Multiple burst writes of length 49152 and bit width 16 has been inferred. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings. _XLX_SEP_ seq  gmem2 stage_2_store activation_accelerator.cpp:1122:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:1122:9: Multiple burst writes of length 49152 and bit width 16 in loop 'stage_2_store'(activation_accelerator.cpp:1122:9) has been inferred on bundle 'gmem2'. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
