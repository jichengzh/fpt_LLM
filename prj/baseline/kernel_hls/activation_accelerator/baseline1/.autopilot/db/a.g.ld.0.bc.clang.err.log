remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/src/c++/expfloat.cpp:22:9: Inlining function 'exp_reduce_::exp(float)' into 'hls::expf(float)'
remark: activation_accelerator.cpp:282:38: Inlining function 'hls::expf(float)' into 'float_safe_softmax(float const*, unsigned short*, int)'
remark: activation_accelerator.cpp:330:20: Inlining function 'hls::expf(float)' into 'float_mask_safe_softmax(float const*, float const*, unsigned short*, int)'
remark: activation_accelerator.cpp:153:36: Inlining function 'hls::expf(float)' into 'float_sigmoid(float const*, unsigned short*, int)'
remark: activation_accelerator.cpp:161:36: Inlining function 'hls::expf(float)' into 'float_silu(float const*, unsigned short*, int)'
remark: activation_accelerator.cpp:307:9: Loop 'normalize_inner' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:279:9: Loop 'exp_inner' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:287:9: Loop 'bucket_add' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:250:9: Loop 'load_blk_max' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:257:9: Loop 'reduce_blk_max' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:87:22: Loop 'VITIS_LOOP_87_1' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:230:0: Unrolling loop 'normalize_inner' (activation_accelerator.cpp:307:9) in function 'float_safe_softmax' completely with a factor of 8
remark: activation_accelerator.cpp:230:0: Unrolling loop 'reduce_partial' (activation_accelerator.cpp:297:5) in function 'float_safe_softmax' completely with a factor of 32
remark: activation_accelerator.cpp:230:0: Unrolling loop 'exp_inner' (activation_accelerator.cpp:279:9) in function 'float_safe_softmax' completely with a factor of 8
remark: activation_accelerator.cpp:230:0: Unrolling loop 'bucket_add' (activation_accelerator.cpp:287:9) in function 'float_safe_softmax' completely with a factor of 8
remark: activation_accelerator.cpp:230:0: Unrolling loop 'init_partial' (activation_accelerator.cpp:267:5) in function 'float_safe_softmax' completely with a factor of 32
remark: activation_accelerator.cpp:230:0: Unrolling loop 'load_blk_max' (activation_accelerator.cpp:250:9) in function 'float_safe_softmax' completely with a factor of 8
remark: activation_accelerator.cpp:230:0: Unrolling loop 'reduce_blk_max' (activation_accelerator.cpp:257:9) in function 'float_safe_softmax' completely with a factor of 8
remark: activation_accelerator.cpp:168:0: Inlining function 'hls::sqrtf(float)' into 'float_rms_norm(float const*, unsigned short*, int)'
remark: activation_accelerator.cpp:183:0: Inlining function 'hls::sqrtf(float)' into 'float_layer_norm(float const*, unsigned short*, int)'
remark: activation_accelerator.cpp:341:0: Inlining function 'bf16_to_float(unsigned short const*, float*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:341:0: Inlining function 'float_add(float const*, float const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:341:0: Inlining function 'float_mask_safe_softmax(float const*, float const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:341:0: Inlining function 'float_sigmoid(float const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:341:0: Inlining function 'float_silu(float const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:341:0: Inlining function 'float_rms_norm(float const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:341:0: Inlining function 'float_layer_norm(float const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:237:11: Applying array_partition to 'exp_x': Cyclic partitioning with factor 8 on dimension 1.
remark: activation_accelerator.cpp:264:11: Applying array_partition to 'partial': Complete partitioning on dimension 1.
remark: activation_accelerator.cpp:87:22: Could not analyze the loop bounds _XLX_SEP_ VITIS_LOOP_87_1 activation_accelerator.cpp:87:22 bf16add(unsigned short, unsigned short) 
remark: activation_accelerator.cpp:355:27: Sequential read of length 32768 has been inferred _XLX_SEP_ OldID=for.inc.load.4,  _XLX_SEP_ in0seq in0 gmem0 VITIS_LOOP_355_1 activation_accelerator.cpp:355:27 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:358:27: Sequential read of length 32768 has been inferred _XLX_SEP_ OldID=for.inc13.load.4,  _XLX_SEP_ in1seq in1 gmem1 VITIS_LOOP_358_2 activation_accelerator.cpp:358:27 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:409:27: Sequential write of length 32768 has been inferred _XLX_SEP_ OldID=for.inc87.store.6,  _XLX_SEP_ outseq out gmem2 VITIS_LOOP_409_4 activation_accelerator.cpp:409:27 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:358:27: Could not widen since type i16 size is greater than or equal to the max_widen_bitwidth threshold of 0 _XLX_SEP_ in1seq in1 gmem1 VITIS_LOOP_358_2 activation_accelerator.cpp:358:27 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:355:27: Could not widen since type i16 size is greater than or equal to the max_widen_bitwidth threshold of 0 _XLX_SEP_ in0seq in0 gmem0 VITIS_LOOP_355_1 activation_accelerator.cpp:355:27 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:409:27: Could not widen since type i16 size is greater than or equal to the max_widen_bitwidth threshold of 0 _XLX_SEP_ outseq out gmem2 VITIS_LOOP_409_4 activation_accelerator.cpp:409:27 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:87:22: Loop 'VITIS_LOOP_87_1' is marked as complete unroll implied by the pipeline pragma
remark: <unknown>:0:0: array_partition dim=1 type=cyclic factor=4 variable=_ZZ22activation_acceleratorPtS_S_iiE4buf2 1 activation_accelerator activation_accelerator.cpp:351:0 activation_accelerator.cpp:305:9activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)::buf2 
remark: activation_accelerator.cpp:305:9: Inferring pragma 'array_partition type=cyclic factor=4 dim=1' for array 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)::buf2' due to pipeline pragma
remark: activation_accelerator.cpp:0:1: array_partition dim=1 type=cyclic factor=4 variable=x 1 activation_accelerator activation_accelerator.cpp:352:11 activation_accelerator.cpp:246:9x 
remark: activation_accelerator.cpp:246:9: Inferring pragma 'array_partition type=cyclic factor=4 dim=1' for array 'x' due to pipeline pragma
remark: activation_accelerator.cpp:351:0: Applying array_partition to '_ZZ22activation_acceleratorPtS_S_iiE4buf2': Cyclic partitioning with factor 4 on dimension 1.
remark: activation_accelerator.cpp:352:11: Applying array_partition to 'x': Cyclic partitioning with factor 4 on dimension 1.
remark: activation_accelerator.cpp:355:27: Multiple burst reads of length 32768 and bit width 16 has been inferred. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings. _XLX_SEP_ seq1  gmem0 VITIS_LOOP_355_1 activation_accelerator.cpp:355:27 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:355:27: Multiple burst reads of length 32768 and bit width 16 in loop 'VITIS_LOOP_355_1'(activation_accelerator.cpp:355:27) has been inferred on bundle 'gmem0'. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
remark: activation_accelerator.cpp:358:27: Multiple burst reads of length 32768 and bit width 16 has been inferred. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings. _XLX_SEP_ seq2  gmem1 VITIS_LOOP_358_2 activation_accelerator.cpp:358:27 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:358:27: Multiple burst reads of length 32768 and bit width 16 in loop 'VITIS_LOOP_358_2'(activation_accelerator.cpp:358:27) has been inferred on bundle 'gmem1'. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
remark: activation_accelerator.cpp:409:27: Multiple burst writes of length 32768 and bit width 16 has been inferred. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings. _XLX_SEP_ seq  gmem2 VITIS_LOOP_409_4 activation_accelerator.cpp:409:27 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:409:27: Multiple burst writes of length 32768 and bit width 16 in loop 'VITIS_LOOP_409_4'(activation_accelerator.cpp:409:27) has been inferred on bundle 'gmem2'. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
remark: activation_accelerator.cpp:87:22: Loop 'VITIS_LOOP_87_1' is marked as complete unroll implied by the pipeline pragma
