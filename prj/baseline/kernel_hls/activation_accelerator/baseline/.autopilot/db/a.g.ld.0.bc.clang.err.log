remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/src/c++/expfloat.cpp:22:9: Inlining function 'exp_reduce_::exp(float)' into 'hls::expf(float)'
remark: activation_accelerator.cpp:111:41: Inlining function 'hls::expf(float)' into 'float_sige(unsigned short const*, unsigned short*, int, float)'
remark: activation_accelerator.cpp:496:24: Inlining function 'hls::expf(float)' into 'float_safe_softmax3(unsigned short const*, unsigned short*, int)'
remark: activation_accelerator.cpp:515:25: Inlining function 'hls::expf(float)' into 'float_safe_softmax3(unsigned short const*, unsigned short*, int)'
remark: activation_accelerator.cpp:677:13: Inlining function 'float_safe_softmax3(unsigned short const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:681:13: Inlining function 'float_layer_norm3(unsigned short const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:685:13: Inlining function 'float_rms_norm3(unsigned short const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:689:13: Inlining function 'float_Multiply2(unsigned short const*, unsigned short const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:693:13: Inlining function 'float_add2(unsigned short const*, unsigned short const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:505:9: Loop 'softmax_final_inner' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:490:9: Loop 'exp_inner_softmax' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:466:20: Loop 'VITIS_LOOP_466_1' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:322:9: Loop 'normalize_inner_layer_norm3' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:293:9: Loop 'mean_inner_layer_norm3' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:244:9: Loop 'normalize_inner_rms_norm3' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:594:9: Loop 'multiply_inner' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:407:9: Loop 'add_inner' is marked as complete unroll implied by the pipeline pragma
remark: ./bf16_accl.h:307:9: Loop 'sum_inner_square' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:614:0: Unrolling loop 'softmax_final_inner' (activation_accelerator.cpp:505:9) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:614:0: Unrolling loop 'exp_inner_softmax' (activation_accelerator.cpp:490:9) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:614:0: Unrolling loop 'init_partial_softmax' (activation_accelerator.cpp:480:5) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:614:0: Unrolling loop 'VITIS_LOOP_466_1' (activation_accelerator.cpp:466:20) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:614:0: Unrolling loop 'init_lane_max_softmax' (activation_accelerator.cpp:457:5) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:614:0: Unrolling loop 'normalize_inner_layer_norm3' (activation_accelerator.cpp:322:9) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:614:0: Unrolling loop 'mean_blocks2_layer_norm3' (activation_accelerator.cpp:302:5) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:614:0: Unrolling loop 'mean_inner_layer_norm3' (activation_accelerator.cpp:293:9) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:614:0: Unrolling loop 'init_partial_layernorm' (activation_accelerator.cpp:278:5) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:614:0: Unrolling loop 'normalize_inner_rms_norm3' (activation_accelerator.cpp:244:9) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:614:0: Unrolling loop 'init_y_sum_and_rms_sq' (activation_accelerator.cpp:223:5) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:614:0: Unrolling loop 'multiply_inner' (activation_accelerator.cpp:594:9) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:614:0: Unrolling loop 'add_inner' (activation_accelerator.cpp:407:9) in function 'activation_accelerator' completely with a factor of 64
remark: ./bf16_accl.h:299:0: Unrolling loop 'sum_square2' (./bf16_accl.h:316:5) in function 'square' completely with a factor of 64
remark: ./bf16_accl.h:299:0: Unrolling loop 'sum_inner_square' (./bf16_accl.h:307:9) in function 'square' completely with a factor of 64
remark: activation_accelerator.cpp:89:0: Unrolling loop 'sige_inner' (activation_accelerator.cpp:102:9) in function 'float_sige' completely with a factor of 32
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_isnan.h:16:0: Inlining function 'fp_struct<float>::fp_struct(float)' into 'int generic_isnan<float>(float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/src/hls/utils/x_hls_utils.h:351:0: Inlining function 'fp_struct<float>::data() const (.48.55.61.69.75.83.89.97.103.111)' into 'fp_struct<float>::to_float() const'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/src/hls/utils/x_hls_utils.h:375:0: Inlining function 'fp_struct<float>::to_float() const' into 'fp_struct<float>::to_ieee() const'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/src/hls/utils/x_hls_utils.h:348:0: Inlining function 'fp_struct<float>::data() const (.48.55.61.69.75.83.89.97.103.111)' into 'fp_struct<float>::to_int() const'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'fp_struct<float>::fp_struct(float)' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'int generic_isnan<float>(float)' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'fp_struct<float>::to_ieee() const' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'fp_struct<float>::to_int() const' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/src/c++/fmaxfloat.cpp:10:0: Inlining function 'hls::fmax(float, float)' into 'hls::fmaxf(float, float)'
remark: activation_accelerator.cpp:614:0: Inlining function 'hls::sqrtf(float)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:624:0: Applying array_partition to '_ZZ22activation_acceleratorPtS_S_iiE4buf2': Block partitioning with factor 64 on dimension 1.
remark: activation_accelerator.cpp:623:0: Applying array_partition to '_ZZ22activation_acceleratorPtS_S_iiE4buf1': Block partitioning with factor 64 on dimension 1.
remark: activation_accelerator.cpp:622:0: Applying array_partition to '_ZZ22activation_acceleratorPtS_S_iiE4buf0': Block partitioning with factor 64 on dimension 1.
remark: activation_accelerator.cpp:213:11: Applying array_partition to 'y_sum_sq.i17': Complete partitioning on dimension 1.
remark: activation_accelerator.cpp:214:11: Applying array_partition to 'rms_sq.i': Complete partitioning on dimension 1.
remark: activation_accelerator.cpp:266:11: Applying array_partition to 'partial_mean.i': Complete partitioning on dimension 1.
remark: activation_accelerator.cpp:267:11: Applying array_partition to 'y_sum_sq.i': Complete partitioning on dimension 1.
remark: activation_accelerator.cpp:659:9: Sequential read of length 49152 has been inferred _XLX_SEP_ OldID=for.inc.load.4,  _XLX_SEP_ in0seq in0 gmem0 stage_0_load0 activation_accelerator.cpp:659:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:663:9: Sequential read of length 49152 has been inferred _XLX_SEP_ OldID=for.inc13.load.4,  _XLX_SEP_ in1seq in1 gmem1 stage_0_load1 activation_accelerator.cpp:663:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:703:9: Sequential write of length 49152 has been inferred _XLX_SEP_ OldID=for.inc55.store.136,  _XLX_SEP_ outseq out gmem2 stage_2_store activation_accelerator.cpp:703:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:654:20: Volatile or Atomic access cannot be transformed _XLX_SEP_ entry.store.921 anchor_reg  activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:663:9: Could not widen since type i16 size is greater than or equal to the max_widen_bitwidth threshold of 0 _XLX_SEP_ in1seq in1 gmem1 stage_0_load1 activation_accelerator.cpp:663:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:659:9: Could not widen since type i16 size is greater than or equal to the max_widen_bitwidth threshold of 0 _XLX_SEP_ in0seq in0 gmem0 stage_0_load0 activation_accelerator.cpp:659:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:703:9: Could not widen since type i16 size is greater than or equal to the max_widen_bitwidth threshold of 0 _XLX_SEP_ outseq out gmem2 stage_2_store activation_accelerator.cpp:703:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:659:9: Multiple burst reads of length 49152 and bit width 16 has been inferred. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings. _XLX_SEP_ seq1  gmem0 stage_0_load0 activation_accelerator.cpp:659:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:659:9: Multiple burst reads of length 49152 and bit width 16 in loop 'stage_0_load0'(activation_accelerator.cpp:659:9) has been inferred on bundle 'gmem0'. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
remark: activation_accelerator.cpp:663:9: Multiple burst reads of length 49152 and bit width 16 has been inferred. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings. _XLX_SEP_ seq2  gmem1 stage_0_load1 activation_accelerator.cpp:663:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:663:9: Multiple burst reads of length 49152 and bit width 16 in loop 'stage_0_load1'(activation_accelerator.cpp:663:9) has been inferred on bundle 'gmem1'. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
remark: activation_accelerator.cpp:703:9: Multiple burst writes of length 49152 and bit width 16 has been inferred. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings. _XLX_SEP_ seq  gmem2 stage_2_store activation_accelerator.cpp:703:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:703:9: Multiple burst writes of length 49152 and bit width 16 in loop 'stage_2_store'(activation_accelerator.cpp:703:9) has been inferred on bundle 'gmem2'. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
