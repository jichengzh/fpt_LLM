remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/src/c++/expfloat.cpp:22:9: Inlining function 'exp_reduce_::exp(float)' into 'hls::expf(float)'
remark: activation_accelerator.cpp:110:41: Inlining function 'hls::expf(float)' into 'float_sige(unsigned short const*, unsigned short*, int, float)'
remark: activation_accelerator.cpp:514:24: Inlining function 'hls::expf(float)' into 'float_safe_softmax3(unsigned short const*, unsigned short*, int)'
remark: activation_accelerator.cpp:536:25: Inlining function 'hls::expf(float)' into 'float_safe_softmax3(unsigned short const*, unsigned short*, int)'
remark: activation_accelerator.cpp:699:13: Inlining function 'float_safe_softmax3(unsigned short const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:703:13: Inlining function 'float_layer_norm3(unsigned short const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:707:13: Inlining function 'float_rms_norm3(unsigned short const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:711:13: Inlining function 'float_Multiply2(unsigned short const*, unsigned short const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:715:13: Inlining function 'float_add2(unsigned short const*, unsigned short const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: ./bf16_accl.h:332:9: Loop 'sum_inner_square' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:636:0: Unrolling loop 'softmax_final_inner' (activation_accelerator.cpp:526:9) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:636:0: Unrolling loop 'exp_inner_softmax' (activation_accelerator.cpp:508:9) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:636:0: Unrolling loop 'init_partial_softmax' (activation_accelerator.cpp:496:5) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:636:0: Unrolling loop 'lane_reduce' (activation_accelerator.cpp:481:9) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:636:0: Unrolling loop 'init_lane_max_softmax' (activation_accelerator.cpp:469:5) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:636:0: Unrolling loop 'normalize_inner_layer_norm3' (activation_accelerator.cpp:325:9) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:636:0: Unrolling loop 'std_blocks_layer_norm3' (activation_accelerator.cpp:311:5) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:636:0: Unrolling loop 'mean_blocks2_layer_norm3' (activation_accelerator.cpp:303:5) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:636:0: Unrolling loop 'mean_inner_layer_norm3' (activation_accelerator.cpp:294:9) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:636:0: Unrolling loop 'init_partial_layernorm' (activation_accelerator.cpp:280:5) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:636:0: Unrolling loop 'normalize_inner_rms_norm3' (activation_accelerator.cpp:246:9) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:636:0: Unrolling loop 'init_y_sum_and_rms_sq' (activation_accelerator.cpp:225:5) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:636:0: Unrolling loop 'multiply_inner' (activation_accelerator.cpp:616:9) in function 'activation_accelerator' completely with a factor of 64
remark: activation_accelerator.cpp:636:0: Unrolling loop 'add_inner' (activation_accelerator.cpp:419:9) in function 'activation_accelerator' completely with a factor of 64
remark: ./bf16_accl.h:324:0: Unrolling loop 'sum_square2' (./bf16_accl.h:341:5) in function 'square' completely with a factor of 64
remark: ./bf16_accl.h:324:0: Unrolling loop 'sum_inner_square' (./bf16_accl.h:332:9) in function 'square' completely with a factor of 64
remark: activation_accelerator.cpp:89:0: Unrolling loop 'sige_inner' (activation_accelerator.cpp:101:9) in function 'float_sige' completely with a factor of 32
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_isnan.h:16:0: Inlining function 'fp_struct<float>::fp_struct(float)' into 'int generic_isnan<float>(float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/src/hls/utils/x_hls_utils.h:351:0: Inlining function 'fp_struct<float>::data() const (.48.55.61.69.75.83.89.97.103.111)' into 'fp_struct<float>::to_float() const'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/src/hls/utils/x_hls_utils.h:375:0: Inlining function 'fp_struct<float>::to_float() const' into 'fp_struct<float>::to_ieee() const'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/src/hls/utils/x_hls_utils.h:348:0: Inlining function 'fp_struct<float>::data() const (.48.55.61.69.75.83.89.97.103.111)' into 'fp_struct<float>::to_int() const'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'fp_struct<float>::fp_struct(float)' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'int generic_isnan<float>(float)' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'fp_struct<float>::to_ieee() const' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'fp_struct<float>::to_int() const' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/src/c++/fmaxfloat.cpp:10:0: Inlining function 'hls::fmax(float, float)' into 'hls::fmaxf(float, float)'
remark: activation_accelerator.cpp:646:0: Applying array_partition to '_ZZ22activation_acceleratorPtS_S_iiE4buf2': Block partitioning with factor 64 on dimension 1.
remark: activation_accelerator.cpp:645:0: Applying array_partition to '_ZZ22activation_acceleratorPtS_S_iiE4buf1': Block partitioning with factor 64 on dimension 1.
remark: activation_accelerator.cpp:644:0: Applying array_partition to '_ZZ22activation_acceleratorPtS_S_iiE4buf0': Block partitioning with factor 64 on dimension 1.
remark: activation_accelerator.cpp:215:11: Applying array_partition to 'y_sum_sq.i17': Complete partitioning on dimension 1.
remark: activation_accelerator.cpp:216:11: Applying array_partition to 'rms_sq.i': Complete partitioning on dimension 1.
remark: activation_accelerator.cpp:268:11: Applying array_partition to 'partial_mean.i': Complete partitioning on dimension 1.
remark: activation_accelerator.cpp:269:11: Applying array_partition to 'y_sum_sq.i': Complete partitioning on dimension 1.
remark: activation_accelerator.cpp:449:7: Applying array_partition to 'sum_row.i': Complete partitioning on dimension 1.
remark: activation_accelerator.cpp:450:7: Applying array_partition to 'max_row.i': Complete partitioning on dimension 1.
remark: activation_accelerator.cpp:725:9: Sequential write of length 49152 has been inferred _XLX_SEP_ OldID=for.inc.store.136,  _XLX_SEP_ outseq out gmem2 stage_2_store activation_accelerator.cpp:725:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:676:20: Volatile or Atomic access cannot be transformed _XLX_SEP_ entry.store.921 anchor_reg  activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:725:9: Could not widen since type i16 size is greater than or equal to the max_widen_bitwidth threshold of 0 _XLX_SEP_ outseq out gmem2 stage_2_store activation_accelerator.cpp:725:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:725:9: Multiple burst writes of length 49152 and bit width 16 has been inferred. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings. _XLX_SEP_ seq  gmem2 stage_2_store activation_accelerator.cpp:725:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:725:9: Multiple burst writes of length 49152 and bit width 16 in loop 'stage_2_store'(activation_accelerator.cpp:725:9) has been inferred on bundle 'gmem2'. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
