remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/src/c++/expfloat.cpp:22:9: Inlining function 'exp_reduce_::exp(float)' into 'hls::expf(float)'
remark: activation_accelerator.cpp:275:29: Inlining function 'hls::expf(float)' into 'float_silu(float const*, ap_uint<16>*, int)'
remark: activation_accelerator.cpp:317:21: Inlining function 'hls::rsqrtf(float)' into 'float_layernorm(float const*, ap_uint<16>*, int)'
remark: activation_accelerator.cpp:350:12: Inlining function 'hls::expf(float)' into 'float_softmax(float const*, ap_uint<16>*, int)'
remark: activation_accelerator.cpp:357:12: Inlining function 'hls::expf(float)' into 'float_softmax(float const*, ap_uint<16>*, int)'
remark: activation_accelerator.cpp:488:22: Inlining function 'pack_word(ap_uint<16> const*)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:471:18: Inlining function 'float_add(float const*, float const*, ap_uint<16>*, int)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:470:23: Inlining function 'float_add(float const*, float const*, ap_uint<16>*, int)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:445:2: Inlining function 'unpack_word(ap_uint<512>, ap_uint<16>*)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:469:24: Inlining function 'float_silu(float const*, ap_uint<16>*, int)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:446:13: Inlining function 'unpack_word(ap_uint<512>, ap_uint<16>*)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:468:27: Inlining function 'float_softmax(float const*, ap_uint<16>*, int)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:458:9: Inlining function 'bf16_to_float(ap_uint<16> const*, float*, int)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:460:13: Inlining function 'bf16_to_float(ap_uint<16> const*, float*, int)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:464:23: Inlining function 'float_add(float const*, float const*, ap_uint<16>*, int)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:465:24: Inlining function 'float_silu(float const*, ap_uint<16>*, int)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:442:18: Loop 'anonymous' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:442:30: Loop 'anonymous' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:64:19: Loop 'VITIS_LOOP_64_1' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:64:19: Loop 'VITIS_LOOP_64_1' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:448:13: Loop 'UNPK_V' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:481:7: Loop 'anonymous' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:484:13: Loop 'PK_V' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:72:22: Loop 'VITIS_LOOP_72_1' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:404:0: Unrolling loop 'anonymous' (activation_accelerator.cpp:442:18) in function 'compute_rows' completely with a factor of 32
remark: activation_accelerator.cpp:404:0: Unrolling loop 'anonymous' (activation_accelerator.cpp:442:30) in function 'compute_rows' completely with a factor of 32
remark: activation_accelerator.cpp:404:0: Unrolling loop 'VITIS_LOOP_64_1' (activation_accelerator.cpp:64:19) in function 'compute_rows' completely with a factor of 32
remark: activation_accelerator.cpp:404:0: Unrolling loop 'VITIS_LOOP_64_1' (activation_accelerator.cpp:64:19) in function 'compute_rows' completely with a factor of 32
remark: activation_accelerator.cpp:404:0: Unrolling loop 'UNPK_V' (activation_accelerator.cpp:448:13) in function 'compute_rows' completely with a factor of 32
remark: activation_accelerator.cpp:404:0: Unrolling loop 'anonymous' (activation_accelerator.cpp:481:7) in function 'compute_rows' completely with a factor of 32
remark: activation_accelerator.cpp:404:0: Unrolling loop 'PK_V' (activation_accelerator.cpp:484:13) in function 'compute_rows' completely with a factor of 32
remark: activation_accelerator.cpp:404:0: Unrolling loop 'VITIS_LOOP_72_1' (activation_accelerator.cpp:72:22) in function 'compute_rows' completely with a factor of 32
remark: activation_accelerator.cpp:301:0: Inlining function 'f32_to_bf16_scalar(float)' into 'float_layernorm(float const*, ap_uint<16>*, int)'
remark: activation_accelerator.cpp:282:0: Inlining function 'Q_rsqrt(float)' into 'float_rmsnorm(float const*, ap_uint<16>*, int)'
remark: activation_accelerator.cpp:282:0: Inlining function 'f32_to_bf16_scalar(float)' into 'float_rmsnorm(float const*, ap_uint<16>*, int)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_isnan.h:16:0: Inlining function 'fp_struct<float>::fp_struct(float)' into 'int generic_isnan<float>(float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/src/hls/utils/x_hls_utils.h:351:0: Inlining function 'fp_struct<float>::data() const (.59.66.72.80.86.94.100.108.114.122)' into 'fp_struct<float>::to_float() const'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/src/hls/utils/x_hls_utils.h:375:0: Inlining function 'fp_struct<float>::to_float() const' into 'fp_struct<float>::to_ieee() const'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/src/hls/utils/x_hls_utils.h:348:0: Inlining function 'fp_struct<float>::data() const (.59.66.72.80.86.94.100.108.114.122)' into 'fp_struct<float>::to_int() const'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'fp_struct<float>::fp_struct(float)' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'int generic_isnan<float>(float)' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'fp_struct<float>::to_ieee() const' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'fp_struct<float>::to_int() const' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/src/c++/fmaxfloat.cpp:10:0: Inlining function 'hls::fmax(float, float)' into 'hls::fmaxf(float, float)'
remark: activation_accelerator.cpp:404:0: Inlining function 'row_op_select(int, int)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:404:0: Inlining function 'f32_to_bf16_scalar(float)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:404:0: Inlining function 'float_layernorm(float const*, ap_uint<16>*, int)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:404:0: Inlining function 'float_rmsnorm(float const*, ap_uint<16>*, int)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:404:0: Inlining function 'hls::fmaxf(float, float)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:412:7: Applying array_partition to 'tile0': Cyclic partitioning with factor 32 on dimension 1.
remark: activation_accelerator.cpp:412:20: Applying array_partition to 'tile1': Cyclic partitioning with factor 32 on dimension 1.
remark: activation_accelerator.cpp:412:33: Applying array_partition to 'tile2': Cyclic partitioning with factor 32 on dimension 1.
remark: activation_accelerator.cpp:424:8: Applying array_partition to 'xt': Cyclic partitioning with factor 32 on dimension 1.
remark: activation_accelerator.cpp:424:18: Applying array_partition to 'yt': Cyclic partitioning with factor 32 on dimension 1.
remark: activation_accelerator.cpp:384:10: Sequential read of length 1536 has been inferred _XLX_SEP_ OldID=for.inc.load.6,  _XLX_SEP_ in0seq in0 gmem0 LOAD_ROW activation_accelerator.cpp:384:5 load_rows(ap_uint<512> const*, ap_uint<512> const*, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&) 
remark: activation_accelerator.cpp:384:10: Sequential read of length 1536 has been inferred _XLX_SEP_ OldID=for.inc.load.12,  _XLX_SEP_ in1seq in1 gmem1 LOAD_ROW activation_accelerator.cpp:384:5 load_rows(ap_uint<512> const*, ap_uint<512> const*, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&) 
remark: activation_accelerator.cpp:499:5: Sequential write of length 1536 has been inferred _XLX_SEP_ OldID=for.inc.store.8,  _XLX_SEP_ outseq out gmem2 STORE_ROW activation_accelerator.cpp:499:5 store_rows(hls::stream<ap_uint<512>, 0>&, ap_uint<512>*) 
remark: activation_accelerator.cpp:384:10: Could not widen since type i512 size is greater than or equal to the max_widen_bitwidth threshold of 0 _XLX_SEP_ in0seq in0 gmem0 LOAD_W activation_accelerator.cpp:391:9 load_rows(ap_uint<512> const*, ap_uint<512> const*, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&) 
remark: activation_accelerator.cpp:384:10: Could not widen since type i512 size is greater than or equal to the max_widen_bitwidth threshold of 0 _XLX_SEP_ in1seq in1 gmem1 LOAD_W activation_accelerator.cpp:391:9 load_rows(ap_uint<512> const*, ap_uint<512> const*, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&) 
remark: activation_accelerator.cpp:499:5: Could not widen since type i512 size is greater than or equal to the max_widen_bitwidth threshold of 0 _XLX_SEP_ outseq out gmem2 STORE_W activation_accelerator.cpp:503:9 store_rows(hls::stream<ap_uint<512>, 0>&, ap_uint<512>*) 
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/src/c++/fmaxfloat.cpp:7:12: Automatically inlining function 'float generic_fmax<float>(float, float)' to improve effectiveness of pipeline pragma in function 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:384:10: Multiple burst reads of length 1536 and bit width 512 has been inferred. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings. _XLX_SEP_ seq  gmem0 LOAD_ROW activation_accelerator.cpp:384:5 load_rows(ap_uint<512> const*, ap_uint<512> const*, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&) (.1) 
remark: activation_accelerator.cpp:384:10: Multiple burst reads of length 1536 and bit width 512 in loop 'LOAD_ROW'(activation_accelerator.cpp:384:5) has been inferred on bundle 'gmem0'. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
remark: activation_accelerator.cpp:384:10: Multiple burst reads of length 1536 and bit width 512 has been inferred. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings. _XLX_SEP_ seq1  gmem1 LOAD_ROW activation_accelerator.cpp:384:5 load_rows(ap_uint<512> const*, ap_uint<512> const*, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&) (.1) 
remark: activation_accelerator.cpp:384:10: Multiple burst reads of length 1536 and bit width 512 in loop 'LOAD_ROW'(activation_accelerator.cpp:384:5) has been inferred on bundle 'gmem1'. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
remark: activation_accelerator.cpp:499:5: Multiple burst writes of length 1536 and bit width 512 has been inferred. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings. _XLX_SEP_ seq  gmem2 STORE_ROW activation_accelerator.cpp:499:5 store_rows(hls::stream<ap_uint<512>, 0>&, ap_uint<512>*) (.1) 
remark: activation_accelerator.cpp:499:5: Multiple burst writes of length 1536 and bit width 512 in loop 'STORE_ROW'(activation_accelerator.cpp:499:5) has been inferred on bundle 'gmem2'. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
