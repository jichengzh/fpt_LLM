remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/src/c++/expfloat.cpp:22:9: Inlining function 'exp_reduce_::exp(float)' into 'hls::expf(float)'
remark: activation_accelerator.cpp:377:16: Inlining function 'hls::expf(float)' into 'float_safe_softmax(float const*, unsigned short*, int)'
remark: activation_accelerator.cpp:442:25: Inlining function 'hls::expf(float)' into 'float_mask_safe_softmax(float const*, float const*, unsigned short*, int)'
remark: activation_accelerator.cpp:158:29: Inlining function 'hls::expf(float)' into 'float_sigmoid(float const*, unsigned short*, int)'
remark: activation_accelerator.cpp:168:29: Inlining function 'hls::expf(float)' into 'float_silu(float const*, unsigned short*, int)'
remark: activation_accelerator.cpp:88:22: Loop 'VITIS_LOOP_88_1' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:469:0: Unrolling loop 'loop_19' (activation_accelerator.cpp:504:13) in function 'activation_accelerator' partially with a factor of 8
remark: activation_accelerator.cpp:275:0: Unrolling loop 'loop_12' (activation_accelerator.cpp:277:5) in function 'float_add' partially with a factor of 8
remark: activation_accelerator.cpp:214:0: Inlining function 'hls::sqrtf(float)' into 'float_rms_norm(float const*, unsigned short*, int)'
remark: activation_accelerator.cpp:240:0: Inlining function 'hls::sqrtf(float)' into 'float_layer_norm(float const*, unsigned short*, int)'
remark: activation_accelerator.cpp:469:0: Inlining function 'bf16_to_float(unsigned short const*, float*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:469:0: Inlining function 'float_add(float const*, float const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:469:0: Inlining function 'float_safe_softmax(float const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:469:0: Inlining function 'float_sigmoid(float const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:469:0: Inlining function 'float_silu(float const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:469:0: Inlining function 'float_rms_norm(float const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:469:0: Inlining function 'float_layer_norm(float const*, unsigned short*, int)' into 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:480:0: Applying array_partition to '_ZZ22activation_acceleratorPtS_S_iiE4buf2': Cyclic partitioning with factor 8 on dimension 1.
remark: activation_accelerator.cpp:479:0: Applying array_partition to '_ZZ22activation_acceleratorPtS_S_iiE4buf1': Cyclic partitioning with factor 8 on dimension 1.
remark: activation_accelerator.cpp:478:0: Applying array_partition to '_ZZ22activation_acceleratorPtS_S_iiE4buf0': Cyclic partitioning with factor 8 on dimension 1.
remark: activation_accelerator.cpp:371:11: Applying array_partition to 'exp_vals.i': Cyclic partitioning with factor 2 on dimension 1.
remark: activation_accelerator.cpp:481:11: Applying array_partition to 'x': Cyclic partitioning with factor 4 on dimension 1.
remark: activation_accelerator.cpp:481:20: Applying array_partition to 'y': Cyclic partitioning with factor 4 on dimension 1.
remark: activation_accelerator.cpp:88:22: Could not analyze the loop bounds _XLX_SEP_ VITIS_LOOP_88_1 activation_accelerator.cpp:88:22 bf16add(unsigned short, unsigned short) 
remark: activation_accelerator.cpp:490:9: Sequential read of length 1024 has been inferred _XLX_SEP_ OldID=for.inc.load.4,  _XLX_SEP_ in0seq in0 gmem0 loop_23 activation_accelerator.cpp:490:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:494:9: Sequential read of length 1024 has been inferred _XLX_SEP_ OldID=for.inc13.load.4,  _XLX_SEP_ in1seq in1 gmem1 loop_24 activation_accelerator.cpp:494:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:569:9: Sequential write of length 1024 has been inferred _XLX_SEP_ OldID=for.inc87.store.24,  _XLX_SEP_ outseq out gmem2 loop_25 activation_accelerator.cpp:569:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:494:9: Could not widen since type i16 size is greater than or equal to the max_widen_bitwidth threshold of 0 _XLX_SEP_ in1seq in1 gmem1 loop_24 activation_accelerator.cpp:494:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:490:9: Could not widen since type i16 size is greater than or equal to the max_widen_bitwidth threshold of 0 _XLX_SEP_ in0seq in0 gmem0 loop_23 activation_accelerator.cpp:490:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:569:9: Could not widen since type i16 size is greater than or equal to the max_widen_bitwidth threshold of 0 _XLX_SEP_ outseq out gmem2 loop_25 activation_accelerator.cpp:569:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:88:22: Loop 'VITIS_LOOP_88_1' is marked as complete unroll implied by the pipeline pragma
error: activation_accelerator.cpp:397:21: in function 'float_mask_safe_softmax(float const*, float const*, unsigned short*, int)': Write internal stream is left dangling
remark: activation_accelerator.cpp:490:9: Multiple burst reads of length 1024 and bit width 16 has been inferred. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings. _XLX_SEP_ seq1  gmem0 loop_23 activation_accelerator.cpp:490:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:490:9: Multiple burst reads of length 1024 and bit width 16 in loop 'loop_23'(activation_accelerator.cpp:490:9) has been inferred on bundle 'gmem0'. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
remark: activation_accelerator.cpp:494:9: Multiple burst reads of length 1024 and bit width 16 has been inferred. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings. _XLX_SEP_ seq2  gmem1 loop_24 activation_accelerator.cpp:494:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:494:9: Multiple burst reads of length 1024 and bit width 16 in loop 'loop_24'(activation_accelerator.cpp:494:9) has been inferred on bundle 'gmem1'. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
remark: activation_accelerator.cpp:569:9: Multiple burst writes of length 1024 and bit width 16 has been inferred. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings. _XLX_SEP_ seq  gmem2 loop_25 activation_accelerator.cpp:569:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:569:9: Multiple burst writes of length 1024 and bit width 16 in loop 'loop_25'(activation_accelerator.cpp:569:9) has been inferred on bundle 'gmem2'. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
remark: activation_accelerator.cpp:88:22: Loop 'VITIS_LOOP_88_1' is marked as complete unroll implied by the pipeline pragma
