remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/src/c++/expfloat.cpp:22:9: Inlining function 'exp_reduce_::exp(float)' into 'hls::expf(float)'
remark: activation_accelerator.cpp:275:29: Inlining function 'hls::expf(float)' into 'float_silu(float const*, ap_uint<16>*, int)'
remark: activation_accelerator.cpp:388:21: Inlining function 'hls::rsqrtf(float)' into 'float_layernorm(float const*, ap_uint<16>*, int)'
remark: activation_accelerator.cpp:537:34: Inlining function 'hls::expf(float)' into 'float_softmax(float const*, ap_uint<16>*, int)'
remark: activation_accelerator.cpp:553:12: Inlining function 'hls::expf(float)' into 'float_softmax(float const*, ap_uint<16>*, int)'
remark: activation_accelerator.cpp:683:22: Inlining function 'pack_word(ap_uint<16> const*)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:666:18: Inlining function 'float_add(float const*, float const*, ap_uint<16>*, int)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:665:23: Inlining function 'float_add(float const*, float const*, ap_uint<16>*, int)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:640:2: Inlining function 'unpack_word(ap_uint<512>, ap_uint<16>*)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:664:24: Inlining function 'float_silu(float const*, ap_uint<16>*, int)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:641:13: Inlining function 'unpack_word(ap_uint<512>, ap_uint<16>*)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:663:27: Inlining function 'float_softmax(float const*, ap_uint<16>*, int)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:653:9: Inlining function 'bf16_to_float(ap_uint<16> const*, float*, int)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:655:13: Inlining function 'bf16_to_float(ap_uint<16> const*, float*, int)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:659:23: Inlining function 'float_add(float const*, float const*, ap_uint<16>*, int)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:660:24: Inlining function 'float_silu(float const*, ap_uint<16>*, int)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:637:18: Loop 'anonymous' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:637:30: Loop 'anonymous' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:64:19: Loop 'VITIS_LOOP_64_1' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:64:19: Loop 'VITIS_LOOP_64_1' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:643:13: Loop 'UNPK_V' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:534:9: Loop 'load_e' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:676:7: Loop 'anonymous' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:679:13: Loop 'PK_V' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:72:22: Loop 'VITIS_LOOP_72_1' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:294:9: Loop 'load_and_sq' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:368:9: Loop 'load_s' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:339:9: Loop 'load_a' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:599:0: Unrolling loop 'anonymous' (activation_accelerator.cpp:637:18) in function 'compute_rows' completely with a factor of 32
remark: activation_accelerator.cpp:599:0: Unrolling loop 'anonymous' (activation_accelerator.cpp:637:30) in function 'compute_rows' completely with a factor of 32
remark: activation_accelerator.cpp:599:0: Unrolling loop 'VITIS_LOOP_64_1' (activation_accelerator.cpp:64:19) in function 'compute_rows' completely with a factor of 32
remark: activation_accelerator.cpp:599:0: Unrolling loop 'VITIS_LOOP_64_1' (activation_accelerator.cpp:64:19) in function 'compute_rows' completely with a factor of 32
remark: activation_accelerator.cpp:599:0: Unrolling loop 'UNPK_V' (activation_accelerator.cpp:643:13) in function 'compute_rows' completely with a factor of 32
remark: activation_accelerator.cpp:599:0: Unrolling loop 'load_e' (activation_accelerator.cpp:534:9) in function 'compute_rows' completely with a factor of 8
remark: activation_accelerator.cpp:599:0: Unrolling loop 'anonymous' (activation_accelerator.cpp:676:7) in function 'compute_rows' completely with a factor of 32
remark: activation_accelerator.cpp:599:0: Unrolling loop 'PK_V' (activation_accelerator.cpp:679:13) in function 'compute_rows' completely with a factor of 32
remark: activation_accelerator.cpp:599:0: Unrolling loop 'VITIS_LOOP_72_1' (activation_accelerator.cpp:72:22) in function 'compute_rows' completely with a factor of 32
remark: activation_accelerator.cpp:282:0: Unrolling loop 'load_and_sq' (activation_accelerator.cpp:294:9) in function 'float_rmsnorm' completely with a factor of 8
remark: activation_accelerator.cpp:324:0: Unrolling loop 'load_s' (activation_accelerator.cpp:368:9) in function 'float_layernorm' completely with a factor of 8
remark: activation_accelerator.cpp:324:0: Unrolling loop 'load_a' (activation_accelerator.cpp:339:9) in function 'float_layernorm' completely with a factor of 8
remark: activation_accelerator.cpp:324:0: Inlining function 'f32_to_bf16_scalar(float)' into 'float_layernorm(float const*, ap_uint<16>*, int)'
remark: activation_accelerator.cpp:282:0: Inlining function 'Q_rsqrt(float)' into 'float_rmsnorm(float const*, ap_uint<16>*, int)'
remark: activation_accelerator.cpp:282:0: Inlining function 'f32_to_bf16_scalar(float)' into 'float_rmsnorm(float const*, ap_uint<16>*, int)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_isnan.h:16:0: Inlining function 'fp_struct<float>::fp_struct(float)' into 'int generic_isnan<float>(float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/src/hls/utils/x_hls_utils.h:351:0: Inlining function 'fp_struct<float>::data() const (.59.66.72.80.86.94.100.108.114.122)' into 'fp_struct<float>::to_float() const'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/src/hls/utils/x_hls_utils.h:375:0: Inlining function 'fp_struct<float>::to_float() const' into 'fp_struct<float>::to_ieee() const'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/src/hls/utils/x_hls_utils.h:348:0: Inlining function 'fp_struct<float>::data() const (.59.66.72.80.86.94.100.108.114.122)' into 'fp_struct<float>::to_int() const'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'fp_struct<float>::fp_struct(float)' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'int generic_isnan<float>(float)' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'fp_struct<float>::to_ieee() const' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'fp_struct<float>::to_int() const' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/src/c++/fmaxfloat.cpp:10:0: Inlining function 'hls::fmax(float, float)' into 'hls::fmaxf(float, float)'
remark: activation_accelerator.cpp:599:0: Inlining function 'row_op_select(int, int)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:599:0: Inlining function 'f32_to_bf16_scalar(float)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:599:0: Inlining function 'hls::fmaxf(float, float)' into 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:607:7: Applying array_partition to 'tile0': Cyclic partitioning with factor 32 on dimension 1.
remark: activation_accelerator.cpp:607:20: Applying array_partition to 'tile1': Cyclic partitioning with factor 32 on dimension 1.
remark: activation_accelerator.cpp:607:33: Applying array_partition to 'tile2': Cyclic partitioning with factor 32 on dimension 1.
remark: activation_accelerator.cpp:619:8: Applying array_partition to 'xt': Cyclic partitioning with factor 32 on dimension 1.
remark: activation_accelerator.cpp:619:18: Applying array_partition to 'yt': Cyclic partitioning with factor 32 on dimension 1.
remark: activation_accelerator.cpp:579:10: Sequential read of length 1536 has been inferred _XLX_SEP_ OldID=for.inc.load.6,  _XLX_SEP_ in0seq in0 gmem0 LOAD_ROW activation_accelerator.cpp:579:5 load_rows(ap_uint<512> const*, ap_uint<512> const*, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&) 
remark: activation_accelerator.cpp:579:10: Sequential read of length 1536 has been inferred _XLX_SEP_ OldID=for.inc.load.12,  _XLX_SEP_ in1seq in1 gmem1 LOAD_ROW activation_accelerator.cpp:579:5 load_rows(ap_uint<512> const*, ap_uint<512> const*, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&) 
remark: activation_accelerator.cpp:694:5: Sequential write of length 1536 has been inferred _XLX_SEP_ OldID=for.inc.store.8,  _XLX_SEP_ outseq out gmem2 STORE_ROW activation_accelerator.cpp:694:5 store_rows(hls::stream<ap_uint<512>, 0>&, ap_uint<512>*) 
remark: activation_accelerator.cpp:579:10: Could not widen since type i512 size is greater than or equal to the max_widen_bitwidth threshold of 0 _XLX_SEP_ in0seq in0 gmem0 LOAD_W activation_accelerator.cpp:586:9 load_rows(ap_uint<512> const*, ap_uint<512> const*, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&) 
remark: activation_accelerator.cpp:579:10: Could not widen since type i512 size is greater than or equal to the max_widen_bitwidth threshold of 0 _XLX_SEP_ in1seq in1 gmem1 LOAD_W activation_accelerator.cpp:586:9 load_rows(ap_uint<512> const*, ap_uint<512> const*, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&) 
remark: activation_accelerator.cpp:694:5: Could not widen since type i512 size is greater than or equal to the max_widen_bitwidth threshold of 0 _XLX_SEP_ outseq out gmem2 STORE_W activation_accelerator.cpp:698:9 store_rows(hls::stream<ap_uint<512>, 0>&, ap_uint<512>*) 
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/src/c++/fmaxfloat.cpp:7:12: Automatically inlining function 'float generic_fmax<float>(float, float)' to improve effectiveness of pipeline pragma in function 'compute_rows(hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&, int)'
remark: activation_accelerator.cpp:579:10: Multiple burst reads of length 1536 and bit width 512 has been inferred. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings. _XLX_SEP_ seq  gmem0 LOAD_ROW activation_accelerator.cpp:579:5 load_rows(ap_uint<512> const*, ap_uint<512> const*, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&) (.1) 
remark: activation_accelerator.cpp:579:10: Multiple burst reads of length 1536 and bit width 512 in loop 'LOAD_ROW'(activation_accelerator.cpp:579:5) has been inferred on bundle 'gmem0'. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
remark: activation_accelerator.cpp:579:10: Multiple burst reads of length 1536 and bit width 512 has been inferred. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings. _XLX_SEP_ seq1  gmem1 LOAD_ROW activation_accelerator.cpp:579:5 load_rows(ap_uint<512> const*, ap_uint<512> const*, hls::stream<ap_uint<512>, 0>&, hls::stream<ap_uint<512>, 0>&) (.1) 
remark: activation_accelerator.cpp:579:10: Multiple burst reads of length 1536 and bit width 512 in loop 'LOAD_ROW'(activation_accelerator.cpp:579:5) has been inferred on bundle 'gmem1'. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
remark: activation_accelerator.cpp:694:5: Multiple burst writes of length 1536 and bit width 512 has been inferred. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings. _XLX_SEP_ seq  gmem2 STORE_ROW activation_accelerator.cpp:694:5 store_rows(hls::stream<ap_uint<512>, 0>&, ap_uint<512>*) (.1) 
remark: activation_accelerator.cpp:694:5: Multiple burst writes of length 1536 and bit width 512 in loop 'STORE_ROW'(activation_accelerator.cpp:694:5) has been inferred on bundle 'gmem2'. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
