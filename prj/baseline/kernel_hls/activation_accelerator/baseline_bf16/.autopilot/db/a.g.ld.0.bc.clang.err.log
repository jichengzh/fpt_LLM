remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/src/c++/expfloat.cpp:22:9: Inlining function 'exp_reduce_::exp(float)' into 'hls::expf(float)'
remark: activation_accelerator.cpp:1177:39: Inlining function 'hls::expf(float)' into 'float row_exp_bucket_sum<32, 32>(float const*, int, float, float*)'
remark: activation_accelerator.cpp:1276:33: Inlining function 'float row_max_hls<32, 32>(float const*, int)' into 'float_safe_softmax3(float const*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:1280:29: Inlining function 'float row_exp_bucket_sum<32, 32>(float const*, int, float, float*)' into 'float_safe_softmax3(float const*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:1283:17: Inlining function 'void row_norm_store_hls<32>(float const*, float, unsigned short*, int)' into 'float_safe_softmax3(float const*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:1119:9: Loop 'load_blk_max' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:1127:9: Loop 'reduce_blk_max' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:1174:9: Loop 'exp_inner' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:1184:9: Loop 'bucket_add' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:1215:9: Loop 'normalize_inner' is marked as complete unroll implied by the pipeline pragma
remark: activation_accelerator.cpp:1102:9: Dependence pragma in loop 'row_unroll_loop' (activation_accelerator.cpp:1265:13) is removed because the loop is unrolled completely
remark: activation_accelerator.cpp:1227:0: Unrolling loop 'row_unroll_loop' (activation_accelerator.cpp:1265:13) in function 'float_safe_softmax3' completely with a factor of 1
remark: activation_accelerator.cpp:1227:0: Unrolling loop 'normalize_inner' (activation_accelerator.cpp:1215:9) in function 'float_safe_softmax3' completely with a factor of 32
remark: activation_accelerator.cpp:1227:0: Unrolling loop 'reduce_partial' (activation_accelerator.cpp:1195:5) in function 'float_safe_softmax3' completely with a factor of 32
remark: activation_accelerator.cpp:1227:0: Unrolling loop 'exp_inner' (activation_accelerator.cpp:1174:9) in function 'float_safe_softmax3' completely with a factor of 32
remark: activation_accelerator.cpp:1227:0: Unrolling loop 'bucket_add' (activation_accelerator.cpp:1184:9) in function 'float_safe_softmax3' completely with a factor of 33
remark: activation_accelerator.cpp:1227:0: Unrolling loop 'final_reduce_max' (activation_accelerator.cpp:1139:5) in function 'float_safe_softmax3' completely with a factor of 31
remark: activation_accelerator.cpp:1227:0: Unrolling loop 'load_blk_max' (activation_accelerator.cpp:1119:9) in function 'float_safe_softmax3' completely with a factor of 32
remark: activation_accelerator.cpp:1227:0: Unrolling loop 'reduce_blk_max' (activation_accelerator.cpp:1127:9) in function 'float_safe_softmax3' completely with a factor of 31
remark: activation_accelerator.cpp:1227:0: Unrolling loop 'init_partial_max' (activation_accelerator.cpp:1106:5) in function 'float_safe_softmax3' completely with a factor of 32
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_case_IEEE754.h:65:0: Inlining function 'fp_struct<float>::fp_struct(float)' into 'unsigned short generic_cast_IEEE754<unsigned short, (ap_q_mode)6, float>(float, bool, hls::enable_if<!(std::numeric_limits<unsigned short>::is_signed), bool>::type)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_case_IEEE754.h:65:0: Inlining function 'fp_struct<float>::mantissa() const' into 'unsigned short generic_cast_IEEE754<unsigned short, (ap_q_mode)6, float>(float, bool, hls::enable_if<!(std::numeric_limits<unsigned short>::is_signed), bool>::type)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_case_IEEE754.h:65:0: Inlining function 'fp_struct<float>::expv() const' into 'unsigned short generic_cast_IEEE754<unsigned short, (ap_q_mode)6, float>(float, bool, hls::enable_if<!(std::numeric_limits<unsigned short>::is_signed), bool>::type)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_case_IEEE754.h:116:0: Inlining function 'unsigned short generic_cast_IEEE754<unsigned short, (ap_q_mode)6, float>(float, bool, hls::enable_if<!(std::numeric_limits<unsigned short>::is_signed), bool>::type)' into 'unsigned short generic_cast_IEEE754<unsigned short, float>(float, bool)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/src/lib_floatconversion.cpp:64:0: Inlining function 'unsigned short generic_cast_IEEE754<unsigned short, float>(float, bool)' into '__hls_fptoui_float_i16'
remark: ./bf16_accl.h:114:0: Inlining function 'pack_bf16(unsigned short, unsigned short, unsigned short)' into 'bf16add_fast(unsigned short, unsigned short)'
remark: ./bf16_accl.h:114:0: Inlining function 'clz32(unsigned int)' into 'bf16add_fast(unsigned short, unsigned short)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_isnan.h:16:0: Inlining function 'fp_struct<float>::fp_struct(float)' into 'int generic_isnan<float>(float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/src/hls/utils/x_hls_utils.h:351:0: Inlining function 'fp_struct<float>::data() const (.48.55.61.69.75.83.89.97.103.111)' into 'fp_struct<float>::to_float() const'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/src/hls/utils/x_hls_utils.h:375:0: Inlining function 'fp_struct<float>::to_float() const' into 'fp_struct<float>::to_ieee() const'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/src/hls/utils/x_hls_utils.h:348:0: Inlining function 'fp_struct<float>::data() const (.48.55.61.69.75.83.89.97.103.111)' into 'fp_struct<float>::to_int() const'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'fp_struct<float>::fp_struct(float)' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'int generic_isnan<float>(float)' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'fp_struct<float>::to_ieee() const' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/include/FloatingPoint/hls_fmax.h:20:0: Inlining function 'fp_struct<float>::to_int() const' into 'float generic_fmax<float>(float, float)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/src/c++/fmaxfloat.cpp:10:0: Inlining function 'hls::fmax(float, float)' into 'hls::fmaxf(float, float)'
remark: activation_accelerator.cpp:1227:0: Inlining function '__hls_fptoui_float_i16' into 'float_safe_softmax3(float const*, unsigned short*, int, int)'
remark: activation_accelerator.cpp:1100:8: Applying array_partition to 'partial_max.i': Complete partitioning on dimension 1.
remark: activation_accelerator.cpp:1235:11: Applying array_partition to 'exp_x': Cyclic partitioning with factor 32 on dimension 2.
remark: activation_accelerator.cpp:1365:9: Sequential read of length 49152 has been inferred _XLX_SEP_ OldID=for.inc.load.4,  _XLX_SEP_ in0seq in0 gmem0 stage_0_load0 activation_accelerator.cpp:1365:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:1421:9: Sequential write of length 49152 has been inferred _XLX_SEP_ OldID=for.inc34.store.6,  _XLX_SEP_ outseq out gmem2 stage_2_store activation_accelerator.cpp:1421:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:1365:9: Could not widen since type i16 size is greater than or equal to the max_widen_bitwidth threshold of 0 _XLX_SEP_ in0seq in0 gmem0 stage_0_load0 activation_accelerator.cpp:1365:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:1421:9: Could not widen since type i16 size is greater than or equal to the max_widen_bitwidth threshold of 0 _XLX_SEP_ outseq out gmem2 stage_2_store activation_accelerator.cpp:1421:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:1133:26: Automatically inlining function 'hls::fmaxf(float, float)' to improve effectiveness of pipeline pragma in function 'float_safe_softmax3(float const*, unsigned short*, int, int)'
remark: /wrk/ci/prod/2022.2/hls_product/continuous/608/2022.2/src/shared/hls/clib/hlsmath/src/c++/fmaxfloat.cpp:7:12: Automatically inlining function 'float generic_fmax<float>(float, float)' to improve effectiveness of pipeline pragma in function 'float_safe_softmax3(float const*, unsigned short*, int, int)'
remark: <unknown>:0:0: array_partition dim=1 type=cyclic factor=16 variable=_ZZ22activation_acceleratorPtS_S_iiE4buf2 1 float_safe_softmax3 activation_accelerator.cpp:1360:0 activation_accelerator.cpp:1213:9activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)::buf2 
remark: activation_accelerator.cpp:1213:9: Inferring pragma 'array_partition type=cyclic factor=16 dim=1' for array 'activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int)::buf2' due to pipeline pragma
remark: activation_accelerator.cpp:0:1: array_partition dim=1 type=cyclic factor=16 variable=x 1 activation_accelerator activation_accelerator.cpp:1361:11 activation_accelerator.cpp:1168:9x 
remark: activation_accelerator.cpp:1168:9: Inferring pragma 'array_partition type=cyclic factor=16 dim=1' for array 'x' due to pipeline pragma
remark: activation_accelerator.cpp:1360:0: Applying array_partition to '_ZZ22activation_acceleratorPtS_S_iiE4buf2': Cyclic partitioning with factor 16 on dimension 1.
remark: activation_accelerator.cpp:1361:11: Applying array_partition to 'x': Cyclic partitioning with factor 16 on dimension 1.
remark: activation_accelerator.cpp:1365:9: Multiple burst reads of length 49152 and bit width 16 has been inferred. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings. _XLX_SEP_ seq1  gmem0 stage_0_load0 activation_accelerator.cpp:1365:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:1365:9: Multiple burst reads of length 49152 and bit width 16 in loop 'stage_0_load0'(activation_accelerator.cpp:1365:9) has been inferred on bundle 'gmem0'. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
remark: activation_accelerator.cpp:1421:9: Multiple burst writes of length 49152 and bit width 16 has been inferred. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings. _XLX_SEP_ seq  gmem2 stage_2_store activation_accelerator.cpp:1421:9 activation_accelerator(unsigned short*, unsigned short*, unsigned short*, int, int) 
remark: activation_accelerator.cpp:1421:9: Multiple burst writes of length 49152 and bit width 16 in loop 'stage_2_store'(activation_accelerator.cpp:1421:9) has been inferred on bundle 'gmem2'. These burst requests might be further partitioned into multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
